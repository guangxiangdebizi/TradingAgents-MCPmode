#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TradingAgents-MCPmode Webå‰ç«¯ - è¶…ç®€åŒ–ç‰ˆæœ¬
åˆ é™¤äº†æœ‰é—®é¢˜çš„æ‘˜è¦å±•å¼€åŠŸèƒ½å’Œé«˜çº§é…ç½®æ¨¡å—
"""

import streamlit as st
import sys
import os
import asyncio
import threading
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ·å¼åŠ è½½å™¨
try:
    from src.web.css_loader import (
        load_financial_css,
        inject_custom_html,
        create_header_html,
        apply_button_style,
        create_section_card_html,
        create_metric_card_html,
    )
except ImportError as e:
    # ç”Ÿäº§ç¯å¢ƒä¸‹å¯èƒ½å› ä¸ºPYTHONPATHæˆ–æ‰“åŒ…æ–¹å¼å¯¼è‡´å¯¼å…¥å¤±è´¥ã€‚
    # æç¤ºä¼šè¢«å…¨å±€CSSéšè—ï¼Œè¿™é‡ŒåŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°ä¾¿äºæ’æŸ¥ã€‚
    print(f"[web_app] CSSæ ·å¼æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    st.error(f"æ— æ³•å¯¼å…¥CSSæ ·å¼æ¨¡å—: {e}")

# å¯¼å…¥å·¥ä½œæµç¨‹ç¼–æ’å™¨
try:
    from src.workflow_orchestrator import WorkflowOrchestrator
except ImportError as e:
    WorkflowOrchestrator = None
    st.error(f"æ— æ³•å¯¼å…¥WorkflowOrchestrator: {e}")

# å¯¼å…¥å¯¼å‡ºå·¥å…·
try:
    from src.dumptools.json_to_markdown import JSONToMarkdownConverter
    from src.dumptools.md2pdf import MarkdownToPDFConverter 
    from src.dumptools.md2docx import MarkdownToDocxConverter
except ImportError as e:
    st.error(f"æ— æ³•å¯¼å…¥å¯¼å‡ºå·¥å…·: {e}")
    JSONToMarkdownConverter = None
    MarkdownToPDFConverter = None
    MarkdownToDocxConverter = None

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIå®éªŒå®¤ - TradingAgents",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# éšè—Streamlitè­¦å‘Šä¿¡æ¯
import warnings
warnings.filterwarnings("ignore")

# éšè—æ§åˆ¶å°æ—¥å¿—
import logging
logging.getLogger().setLevel(logging.ERROR)

# éšè—Streamlitçš„ä¸€äº›UIå…ƒç´ 
try:
    st.set_option('client.showErrorDetails', False)
    st.set_option('client.toolbarMode', 'minimal')
except:
    pass

# æ·»åŠ CSSéšè—ä¸éœ€è¦çš„å…ƒç´ 
st.markdown("""
<style>
/* éšè—æˆåŠŸæç¤ºæ¡† */
.stAlert[data-testid="stAlertContainer"] {
    display: none !important;
}

/* éšè—è­¦å‘Šæç¤ºæ¡† */
.stAlert {
    display: none !important;
}

/* éšè—æ‰€æœ‰é€šçŸ¥ */
[data-baseweb="notification"] {
    display: none !important;
}

/* éšè—Streamlitçš„é»˜è®¤è­¦å‘Š */
.stException {
    display: none !important;
}
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "orchestrator" not in st.session_state:
    st.session_state.orchestrator = None
if "analysis_running" not in st.session_state:
    st.session_state.analysis_running = False
if "selected_session_file" not in st.session_state:
    st.session_state.selected_session_file = None
if "current_session_data" not in st.session_state:
    st.session_state.current_session_data = None
if "analysis_completed" not in st.session_state:
    st.session_state.analysis_completed = False


def load_page_styles():
    """åŠ è½½é¡µé¢æ ·å¼"""
    try:
        load_financial_css()
        inject_custom_html()
        try:
            apply_button_style()
        except Exception:
            pass
    except:
        pass


def render_top_header():
    """å°†æŠ¬å¤´ç»„ä»¶ç´§è´´é¡µé¢æœ€ä¸Šæ–¹ï¼ˆå»æ‰Streamlité»˜è®¤headerä¸é¡¶éƒ¨ç•™ç™½ï¼‰ã€‚"""
    # ç§»é™¤ Streamlit è‡ªå¸¦ header å ä½ï¼Œå¹¶å‹ç¼©ä¸»å®¹å™¨çš„é¡¶éƒ¨å†…è¾¹è·
    st.markdown(
        """
<style>
header { display: none !important; }
.main .block-container { padding-top: 0 !important; }
.header-container { margin-top: 0 !important; }
</style>
        """,
        unsafe_allow_html=True,
    )
    try:
        st.markdown(create_header_html(), unsafe_allow_html=True)
    except Exception as e:
        # å›é€€åˆ°åŸç”Ÿæ ‡é¢˜ï¼Œé¿å…å…¬ç½‘ç¯å¢ƒæŠ¬å¤´ç¼ºå¤±
        print(f"[web_app] æ¸²æŸ“è‡ªå®šä¹‰æŠ¬å¤´å¤±è´¥ï¼Œä½¿ç”¨fallback: {e}")
        st.title("TradingAgents-MCPmode")
        st.caption("åŸºäºMCPå·¥å…·çš„å¤šæ™ºèƒ½ä½“äº¤æ˜“åˆ†æç³»ç»Ÿ")


@st.cache_data(ttl=5)
def get_session_files_list():
    """è·å–ä¼šè¯æ–‡ä»¶åˆ—è¡¨"""
    try:
        dump_dir = Path("src/dump")
        if not dump_dir.exists():
            return []
        return sorted(dump_dir.glob("session_*.json"), key=lambda f: f.stat().st_mtime, reverse=True)
    except:
        return []


def get_agent_display_name(agent_name):
    """è·å–æ™ºèƒ½ä½“æ˜¾ç¤ºåç§°"""
    name_mapping = {
        'company_overview_analyst': 'ğŸ¢ å…¬å¸æ¦‚è¿°åˆ†æå¸ˆ',
        'market_analyst': 'ğŸ“ˆ å¸‚åœºåˆ†æå¸ˆ',
        'sentiment_analyst': 'ğŸ˜Š æƒ…ç»ªåˆ†æå¸ˆ',
        'news_analyst': 'ğŸ“° æ–°é—»åˆ†æå¸ˆ',
        'fundamentals_analyst': 'ğŸ“‹ åŸºæœ¬é¢åˆ†æå¸ˆ',
        'shareholder_analyst': 'ğŸ‘¥ è‚¡ä¸œåˆ†æå¸ˆ',
        'product_analyst': 'ğŸ­ äº§å“åˆ†æå¸ˆ',
        'bull_researcher': 'ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜',
        'bear_researcher': 'ğŸ» çœ‹è·Œç ”ç©¶å‘˜',
        'research_manager': 'ğŸ‘” ç ”ç©¶ç»ç†',
        'trader': 'ğŸ’¼ äº¤æ˜“å‘˜',
        'aggressive_risk_analyst': 'âš¡ æ¿€è¿›é£é™©åˆ†æå¸ˆ',
        'safe_risk_analyst': 'ğŸ›¡ï¸ ä¿å®ˆé£é™©åˆ†æå¸ˆ',
        'neutral_risk_analyst': 'âš–ï¸ ä¸­æ€§é£é™©åˆ†æå¸ˆ',
        'risk_manager': 'ğŸ¯ é£é™©ç»ç†'
    }
    return name_mapping.get(agent_name, agent_name)


async def connect_orchestrator_async():
    """å¼‚æ­¥è¿æ¥WorkflowOrchestrator"""
    if WorkflowOrchestrator is None:
        return False
    
    try:
        load_dotenv()
        orchestrator = WorkflowOrchestrator()
        
        # ğŸ”‘ å…³é”®æ­¥éª¤ï¼šæŒ‰ç…§main.pyçš„æ–¹å¼æ­£ç¡®åˆå§‹åŒ–MCPè¿æ¥
        print("æ­£åœ¨åˆå§‹åŒ–MCPè¿æ¥...")
        await orchestrator.initialize()
        
        # è·å–å·¥å…·ä¿¡æ¯éªŒè¯è¿æ¥æˆåŠŸ
        workflow_info = orchestrator.get_workflow_info()
        tools_count = workflow_info['mcp_tools_info']['total_tools']
        print(f"âœ… æˆåŠŸè¿æ¥åˆ°MCPæœåŠ¡å™¨ï¼Œå‘ç° {tools_count} ä¸ªå·¥å…·")
        
        st.session_state.orchestrator = orchestrator
        return True
    except Exception as e:
        print(f"è¿æ¥å¤±è´¥: {e}")
        return False


def connect_orchestrator():
    """è¿æ¥WorkflowOrchestrator - åŒæ­¥åŒ…è£…å™¨"""
    try:
        # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥å¤„ç†æ–¹å¼
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºæ–°çš„çº¿ç¨‹è¿è¡Œ
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(lambda: asyncio.run(connect_orchestrator_async()))
                    result = future.result(timeout=30)
                    return result
            else:
                return loop.run_until_complete(connect_orchestrator_async())
        except RuntimeError:
            return asyncio.run(connect_orchestrator_async())
    except Exception as e:
        print(f"è¿æ¥å¤±è´¥: {e}")
        return False


async def get_system_capabilities_async():
    """å¼‚æ­¥è·å–ç³»ç»Ÿèƒ½åŠ›ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿ç•™ç®€åŒ–ç‰ˆæœ¬ï¼‰ã€‚"""
    try:
        temp_orchestrator = WorkflowOrchestrator()
        await temp_orchestrator.initialize()
        info = temp_orchestrator.get_workflow_info()
        await temp_orchestrator.close()
        return info
    except Exception:
        return None


@st.cache_data(ttl=30)
def get_system_capabilities():
    """è·å–ç³»ç»Ÿèƒ½åŠ›ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿ç•™ï¼Œä½œä¸ºåº•éƒ¨æ¦‚è§ˆä½¿ç”¨ï¼‰ã€‚"""
    try:
        if st.session_state.get('orchestrator'):
            return st.session_state.orchestrator.get_workflow_info()
        return asyncio.run(get_system_capabilities_async())
    except Exception:
        return {'agents_count': 15, 'mcp_tools_info': {'total_tools': 0, 'server_count': 1, 'servers': {}, 'agent_permissions': {}}}


def show_system_overview():
    """æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ"""
    st.markdown("### ğŸ›ï¸ AIäº¤æ˜“åˆ†æå®éªŒå®¤")
    
    # è·å–ç³»ç»Ÿèƒ½åŠ›ä¿¡æ¯
    capabilities = get_system_capabilities()
    
    if capabilities and capabilities.get('mcp_tools_info'):
        # åˆ›å»ºæ¦‚è§ˆå¡ç‰‡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            mcp_info = capabilities.get('mcp_tools_info', {})
            total_tools = mcp_info.get('total_tools', 0)
            st.metric("ğŸ”§ MCPå·¥å…·æ€»æ•°", total_tools if total_tools > 0 else "è¿æ¥ä¸­...")
        
        with col2:
            server_count = mcp_info.get('server_count', 0)
            st.metric("ğŸ–¥ï¸ MCPæœåŠ¡å™¨", server_count if server_count > 0 else "1")
        
        with col3:
            agents_count = capabilities.get('agents_count', 0)
            st.metric("ğŸ¤– æ™ºèƒ½ä½“æ€»æ•°", agents_count if agents_count > 0 else "15")
        
        with col4:
            enabled_agents = len([agent for agent, enabled in mcp_info.get('agent_permissions', {}).items() if enabled])
            st.metric("âœ… å¯ç”¨MCPæƒé™", enabled_agents if enabled_agents > 0 else "9")
        
        # æ˜¾ç¤ºè¯¦ç»†å·¥å…·ä¿¡æ¯
        if total_tools > 0:
            with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†å·¥å…·ä¿¡æ¯", expanded=False):
                servers_info = mcp_info.get('servers', {})
                for server_name, server_data in servers_info.items():
                    st.markdown(f"**{server_name}** ({server_data.get('tool_count', 0)} ä¸ªå·¥å…·)")
                    tools = server_data.get('tools', [])
                    for tool in tools[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå·¥å…·
                        tool_desc = tool.get('description', 'æ— æè¿°')[:50] + ('...' if len(tool.get('description', '')) > 50 else '')
                        st.markdown(f"  - `{tool.get('name', 'æœªçŸ¥')}`: {tool_desc}")
                    if len(tools) > 5:
                        st.markdown(f"  - ... è¿˜æœ‰ {len(tools) - 5} ä¸ªå·¥å…·")
        else:
            # å¦‚æœå·¥å…·æ•°é‡ä¸º0ï¼Œæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            with st.expander("ğŸ”§ MCPè¿æ¥çŠ¶æ€è°ƒè¯•", expanded=True):
                if st.session_state.get('orchestrator'):
                    orchestrator = st.session_state.orchestrator
                    if hasattr(orchestrator, 'mcp_manager'):
                        mcp_manager = orchestrator.mcp_manager
                        st.write(f"MCPå®¢æˆ·ç«¯çŠ¶æ€: {'å·²è¿æ¥' if mcp_manager.client else 'æœªè¿æ¥'}")
                        st.write(f"å·¥å…·åˆ—è¡¨é•¿åº¦: {len(mcp_manager.tools)}")
                        st.write(f"æŒ‰æœåŠ¡å™¨åˆ†ç»„çš„å·¥å…·: {len(mcp_manager.tools_by_server)}")
                        if mcp_manager.tools:
                            st.write("å‘ç°çš„å·¥å…·:")
                            for i, tool in enumerate(mcp_manager.tools[:3]):
                                st.write(f"  - {tool.name}: {tool.description}")
                        
                        # æ˜¾ç¤ºè¿æ¥çš„æœåŠ¡å™¨ä¿¡æ¯
                        st.write(f"é…ç½®çš„æœåŠ¡å™¨: {list(mcp_manager.config.get('servers', {}).keys())}")
                else:
                    st.warning("å°šæœªè¿æ¥WorkflowOrchestrator")
        
        # æ˜¾ç¤ºæ™ºèƒ½ä½“æƒé™çŠ¶æ€
        with st.expander("ğŸ‘¥ æ™ºèƒ½ä½“MCPæƒé™çŠ¶æ€", expanded=False):
            permissions = mcp_info.get('agent_permissions', {})
            
            # æŒ‰å›¢é˜Ÿåˆ†ç»„æ˜¾ç¤º
            teams = {
                'ğŸ“Š åˆ†æå¸ˆå›¢é˜Ÿ': ['company_overview_analyst', 'market_analyst', 'sentiment_analyst', 'news_analyst', 'fundamentals_analyst', 'shareholder_analyst', 'product_analyst'],
                'ğŸ”¬ ç ”ç©¶å‘˜å›¢é˜Ÿ': ['bull_researcher', 'bear_researcher'],
                'ğŸ‘” ç®¡ç†å±‚': ['research_manager', 'trader'],
                'âš–ï¸ é£é™©ç®¡ç†å›¢é˜Ÿ': ['aggressive_risk_analyst', 'safe_risk_analyst', 'neutral_risk_analyst', 'risk_manager']
            }
            
            for team_name, team_agents in teams.items():
                st.markdown(f"**{team_name}**")
                team_cols = st.columns(len(team_agents))
                for i, agent in enumerate(team_agents):
                    with team_cols[i]:
                        status = "âœ…" if permissions.get(agent, False) else "âŒ"
                        agent_display = get_agent_display_name(agent)
                        st.markdown(f"{status} {agent_display}", help=f"{agent}: {'å¯ç”¨' if permissions.get(agent, False) else 'ç¦ç”¨'}")
    else:
        # å¦‚æœæ— æ³•è·å–ç³»ç»Ÿä¿¡æ¯ï¼Œæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        st.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ¤– æ™ºèƒ½ä½“æ€»æ•°", "15")
        with col2:
            st.metric("âœ… å¯ç”¨MCPæƒé™", "9")
        with col3:
            st.metric("ğŸ”§ MCPå·¥å…·æ•°", "æ£€æµ‹ä¸­...")
        with col4:
            st.metric("ğŸ–¥ï¸ MCPæœåŠ¡å™¨", "1")
    
    st.markdown("---")


def show_real_time_analysis():
    """å®æ—¶åˆ†ææ¨¡å— - è‡ªåŠ¨è¿æ¥ç‰ˆæœ¬"""
    if WorkflowOrchestrator is None:
        st.error("ğŸ˜± æ— æ³•åŠ è½½WorkflowOrchestratorï¼Œè¯·æ£€æŸ¥åç«¯é…ç½®")
        return
    
    # è‡ªåŠ¨è¿æ¥ç³»ç»Ÿï¼ˆå¦‚æœæœªè¿æ¥ï¼‰
    if not st.session_state.get('orchestrator'):
        if connect_orchestrator():
            st.session_state.auto_connected = True
    
    # ç®€åŒ–çš„è¾“å…¥å’Œæ§åˆ¶
    query = st.text_input(
        "è¾“å…¥æŸ¥è¯¢",
        placeholder="ä¾‹å¦‚ï¼šç»™æˆ‘åˆ†æä¸€ä¸‹600833å§",
        key="analysis_query_simple"
    )
    
    # ç®€åŒ–çš„æŒ‰é’®å¸ƒå±€ - åªæ˜¾ç¤ºå¼€å§‹/åœæ­¢
    btn_col1, btn_col2 = st.columns(2)
    
    with btn_col1:
        if st.session_state.analysis_running:
            if st.button("â¹ï¸ åœæ­¢åˆ†æ", use_container_width=True):
                stop_analysis()
        else:
            orchestrator_connected = st.session_state.get('orchestrator') is not None
            analysis_disabled = not query or not orchestrator_connected
            if st.button("ğŸš€ å¼€å§‹åˆ†æ", disabled=analysis_disabled, use_container_width=True):
                if query:
                    start_analysis(query)
    
    with btn_col2:
        # ä¸åœ¨æ­¤å¤„æ˜¾ç¤ºè¿›åº¦ï¼›åªæç¤ºæŸ¥çœ‹â€œå½“å‰ä»»åŠ¡è¿›åº¦â€æ¨¡å—
        if st.session_state.get('analysis_running') or st.session_state.get('analysis_completed'):
            st.caption("è¿›åº¦å·²ç§»åŠ¨åˆ°ä¸‹æ–¹ â€˜å½“å‰ä»»åŠ¡è¿›åº¦â€™ æ¨¡å—æŸ¥çœ‹")
        else:
            if st.session_state.get('orchestrator'):
                st.success("ğŸŸ¢ ç³»ç»Ÿå·²å°±ç»ª")
            else:
                st.error("ğŸ”´ ç³»ç»Ÿæœªè¿æ¥")
    
    # å®Œæˆæç¤º
    if st.session_state.analysis_completed:
        st.success("âœ… åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹ç»“æœã€‚")


def show_history_management():
    """å†å²ä¼šè¯ç®¡ç† - è¶…ç®€åŒ–ç‰ˆæœ¬"""
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = get_session_files_list()
    if not json_files:
        st.info("ğŸ“­ æš‚æ— å†å²åˆ†ææ•°æ®")
        return
    
    # åªçº³å…¥å·²å®Œæˆä»»åŠ¡çš„ä¼šè¯ï¼›æ ‡ç­¾æ˜¾ç¤ºç”¨æˆ·é—®é¢˜è€Œéæ–‡ä»¶å
    completed_files = []
    file_options = []
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if (data.get('status') or '').lower() != 'completed':
                continue
            file_time = datetime.fromtimestamp(json_file.stat().st_mtime)
            time_str = file_time.strftime('%m-%d %H:%M')
            user_query = (data.get('user_query') or '').strip()
            label = f"(æ— æŸ¥è¯¢) - {time_str}" if not user_query else f"{(user_query[:40] + '...') if len(user_query) > 40 else user_query} - {time_str}"
            completed_files.append(json_file)
            file_options.append(label)
        except Exception:
            continue

    if not completed_files:
        st.info("ğŸ“ æš‚æ— å·²å®Œæˆçš„å†å²ä¼šè¯")
        return
    
    # è®°å¿†é€‰ä¸­é¡¹ç´¢å¼•
    if "history_selected_index" not in st.session_state:
        st.session_state.history_selected_index = 0
    
    def on_session_change():
        """ä¼šè¯é€‰æ‹©å˜åŒ–æ—¶è‡ªåŠ¨åŠ è½½"""
        selected_idx = st.session_state.history_selector_simple
        if selected_idx < len(completed_files):
            selected_file = str(completed_files[selected_idx])
            load_session_data(selected_file)
            st.session_state.history_selected_index = selected_idx
    
    selected_index = st.selectbox(
        "é€‰æ‹©å†å²ä¼šè¯",
        range(len(file_options)),
        index=min(st.session_state.history_selected_index, len(file_options) - 1),
        format_func=lambda i: file_options[i],
        key="history_selector_simple",
        on_change=on_session_change
    )
    
    # é™é»˜åŠ è½½ä¼šè¯ä¿¡æ¯ï¼Œä¸æ˜¾ç¤ºæç¤º
    if st.session_state.current_session_data:
        # é™é»˜å¤„ç†ï¼Œä¸æ˜¾ç¤ºä»»ä½•æç¤º
        pass


def show_export_options():
    """å¯¼å‡ºé€‰é¡¹ - è¶…ç®€åŒ–ç‰ˆæœ¬"""
    if not st.session_state.current_session_data or not st.session_state.selected_session_file:
        st.info("è¯·å…ˆåŠ è½½ä¼šè¯æ•°æ®")
        return
    
    # ç®€åŒ–çš„å¯¼å‡ºæŒ‰é’®
    export_col1, export_col2, export_col3 = st.columns(3)
    
    with export_col1:
        if st.button("ğŸ“„ å¯¼å‡ºMD", key="export_md_simple"):
            export_to_markdown()
    
    with export_col2:
        if st.button("ğŸ“„ å¯¼å‡ºPDF", key="export_pdf_simple"):
            export_to_pdf()
    
    with export_col3:
        if st.button("ğŸ“„ å¯¼å‡ºWord", key="export_word_simple"):
            export_to_docx()


def show_analysis_results():
    """åˆ†æç»“æœå±•ç¤º - ç®€åŒ–ç‰ˆæœ¬"""
    if not st.session_state.current_session_data:
        st.info("è¯·å…ˆè¿è¡Œåˆ†ææˆ–åŠ è½½å†å²ä¼šè¯æŸ¥çœ‹ç»“æœ")
        return
    
    data = st.session_state.current_session_data
    
    # æ˜¾ç¤ºä¼šè¯åŸºæœ¬ä¿¡æ¯
    info_col1, info_col2, info_col3 = st.columns(3)
    with info_col1:
        st.metric("ä¼šè¯ID", data.get('session_id', 'N/A')[:8] + "...")
    with info_col2:
        st.metric("çŠ¶æ€", data.get('status', 'N/A'))
    with info_col3:
        completed_agents = len([agent for agent in data.get('agents', []) if agent.get('status') == 'completed'])
        st.metric("å®Œæˆæ™ºèƒ½ä½“", f"{completed_agents}/{len(data.get('agents', []))}")
    
    # æ˜¾ç¤ºç”¨æˆ·æŸ¥è¯¢
    if data.get('user_query'):
        st.markdown("**ğŸ” åˆ†ææŸ¥è¯¢:**")
        st.info(data['user_query'])
    
    # æ™ºèƒ½ä½“ç»“æœæ ‡ç­¾é¡µ
    if data.get('agents'):
        completed_agents = [agent for agent in data['agents'] if agent.get('status') == 'completed']
        
        if completed_agents:
            # æŒ‰æ™ºèƒ½ä½“ç±»å‹åˆ†ç»„
            agent_groups = {
                "ğŸ“Š åˆ†æå¸ˆå›¢é˜Ÿ": ['company_overview_analyst', 'market_analyst', 'sentiment_analyst', 
                            'news_analyst', 'fundamentals_analyst', 'shareholder_analyst', 'product_analyst'],
                "ğŸ”„ çœ‹æ¶¨çœ‹è·Œè¾©è®º": ['bull_researcher', 'bear_researcher'],
                "ğŸ‘” ç ”ç©¶ä¸äº¤æ˜“": ['research_manager', 'trader'],
                "âš–ï¸ é£é™©ç®¡ç†": ['aggressive_risk_analyst', 'safe_risk_analyst', 'neutral_risk_analyst', 'risk_manager']
            }
            
            group_tabs = st.tabs(list(agent_groups.keys()))
            
            for tab_idx, (group_name, agent_names) in enumerate(agent_groups.items()):
                with group_tabs[tab_idx]:
                    group_agents = [agent for agent in completed_agents if agent.get('agent_name') in agent_names]
                    
                    if group_agents:
                        for agent in group_agents:
                            show_agent_result(agent)
                    else:
                        st.info(f"{group_name.split(' ', 1)[1]}æš‚æ— å®Œæˆçš„åˆ†æç»“æœ")
        else:
            st.info("è¯¥ä¼šè¯ä¸­æš‚æ— å®Œæˆçš„æ™ºèƒ½ä½“åˆ†æç»“æœ")
    else:
        st.info("è¯¥ä¼šè¯ä¸­æš‚æ— æ™ºèƒ½ä½“æ•°æ®")


def show_agent_result(agent: Dict[str, Any]):
    """æ˜¾ç¤ºå•ä¸ªæ™ºèƒ½ä½“ç»“æœ - ç®€æ´ç›´æ¥æ¨¡å¼ï¼Œä¸æå¤æ‚çš„æ‘˜è¦å±•å¼€"""
    agent_name = agent.get('agent_name', 'Unknown')
    display_name = get_agent_display_name(agent_name)
    result_content = agent.get('result', '')
    
    if not result_content:
        with st.expander(display_name, expanded=False):
            st.info("è¯¥æ™ºèƒ½ä½“æš‚æ— åˆ†æç»“æœ")
        return
    
    # ç›´æ¥æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œåˆ é™¤æœ‰é—®é¢˜çš„æ‘˜è¦å±•å¼€åŠŸèƒ½
    with st.expander(display_name, expanded=False):
        st.markdown(result_content)


# å¯¼å‡ºåŠŸèƒ½
def export_to_markdown():
    """å¯¼å‡ºMarkdown"""
    if not JSONToMarkdownConverter:
        st.error("âŒ Markdownå¯¼å‡ºå™¨ä¸å¯ç”¨")
        return
    
    try:
        converter = JSONToMarkdownConverter("src/dump")
        result = converter.convert_json_to_markdown(st.session_state.selected_session_file)
        if result and os.path.exists(result):
            st.success(f"âœ… Markdownå¯¼å‡ºæˆåŠŸ: {result}")
            
            # æä¾›ä¸‹è½½é“¾æ¥
            with open(result, 'r', encoding='utf-8') as f:
                content = f.read()
            
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½Markdownæ–‡ä»¶",
                data=content,
                file_name=f"{Path(result).name}",
                mime="text/markdown"
            )
        else:
            st.error("âŒ Markdownå¯¼å‡ºå¤±è´¥")
    except Exception as e:
        st.error(f"âŒ å¯¼å‡ºé”™è¯¯: {str(e)}")


def export_to_pdf():
    """å¯¼å‡ºPDF"""
    if not MarkdownToPDFConverter:
        st.error("âŒ PDFå¯¼å‡ºå™¨ä¸å¯ç”¨")
        return
    
    try:
        converter = MarkdownToPDFConverter("src/dump")
        result = converter.convert_json_to_pdf_via_markdown(st.session_state.selected_session_file)
        if result and os.path.exists(result):
            st.success(f"âœ… PDFå¯¼å‡ºæˆåŠŸ: {result}")
            
            # æä¾›ä¸‹è½½é“¾æ¥
            with open(result, 'rb') as f:
                content = f.read()
            
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½PDFæ–‡ä»¶",
                data=content,
                file_name=f"{Path(result).name}",
                mime="application/pdf"
            )
        else:
            st.error("âŒ PDFå¯¼å‡ºå¤±è´¥")
    except Exception as e:
        st.error(f"âŒ PDFå¯¼å‡ºé”™è¯¯: {str(e)}")


def export_to_docx():
    """å¯¼å‡ºWordæ–‡æ¡£"""
    if not MarkdownToDocxConverter:
        st.error("âŒ DOCXå¯¼å‡ºå™¨ä¸å¯ç”¨")
        return
    
    try:
        converter = MarkdownToDocxConverter("src/dump")
        result = converter.convert_json_to_docx_via_markdown(st.session_state.selected_session_file)
        if result and os.path.exists(result):
            st.success(f"âœ… DOCXå¯¼å‡ºæˆåŠŸ: {result}")
            
            # æä¾›ä¸‹è½½é“¾æ¥
            with open(result, 'rb') as f:
                content = f.read()
            
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½Wordæ–‡ä»¶",
                data=content,
                file_name=f"{Path(result).name}",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        else:
            st.error("âŒ DOCXå¯¼å‡ºå¤±è´¥")
    except Exception as e:
        st.error(f"âŒ DOCXå¯¼å‡ºé”™è¯¯: {str(e)}")


def load_session_data(json_file_path: str):
    """åŠ è½½ä¼šè¯æ•°æ®"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            session_data = json.load(f)
        st.session_state.selected_session_file = json_file_path
        st.session_state.current_session_data = session_data
        # é™é»˜åŠ è½½ï¼Œä¸æ˜¾ç¤ºä»»ä½•æç¤ºï¼Œä¸è°ƒç”¨st.rerun()
    except Exception as e:
        # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸åœ¨å‰ç«¯æ˜¾ç¤º
        print(f"åŠ è½½å¤±è´¥: {str(e)}")


@st.cache_data(ttl=2)
def get_real_analysis_progress():
    """ä»çœŸå®çš„ä¼šè¯JSONæ–‡ä»¶è·å–è¿›åº¦"""
    try:
        dump_dir = Path("src/dump")
        if not dump_dir.exists():
            return None
            
        # æŸ¥æ‰¾æœ€æ–°çš„ä¼šè¯æ–‡ä»¶
        session_files = list(dump_dir.glob("session_*.json"))
        if not session_files:
            return None
            
        latest_session = max(session_files, key=lambda f: f.stat().st_mtime)
        
        # è§£æä¼šè¯è¿›åº¦
        with open(latest_session, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        agents = data.get('agents', [])
        total_agents = 15  # æ€»å…±15ä¸ªæ™ºèƒ½ä½“
        completed_agents = len([a for a in agents if a.get('status') == 'completed'])
        
        progress = (completed_agents / total_agents) * 100 if total_agents > 0 else 0
        
        # ç”ŸæˆçŠ¶æ€æè¿°
        if data.get('status') == 'completed':
            status = "åˆ†æå®Œæˆ"
        elif data.get('status') == 'cancelled':
            status = "åˆ†æå·²å–æ¶ˆ"
        elif completed_agents == 0:
            status = "æ­£åœ¨åˆå§‹åŒ–..."
        else:
            running_agent = next((a for a in agents if a.get('status') == 'running'), None)
            if running_agent:
                agent_name = running_agent.get('agent_name', 'æœªçŸ¥æ™ºèƒ½ä½“')
                display_name = get_agent_display_name(agent_name)
                status = f"æ­£åœ¨æ‰§è¡Œ: {display_name}"
            else:
                status = f"å·²å®Œæˆ {completed_agents} ä¸ªæ™ºèƒ½ä½“"
        
        return {
            'progress': progress,
            'status': status,
            'completed_agents': completed_agents,
            'total_agents': total_agents,
            'session_file': str(latest_session)
        }
        
    except Exception as e:
        return None


@st.cache_data(ttl=2)
def get_all_sessions_progress():
    """æ‰«ææ‰€æœ‰ä¼šè¯æ–‡ä»¶ï¼Œè¿”å›è¿›åº¦æ±‡æ€»åˆ—è¡¨ã€‚"""
    sessions_info: List[Dict[str, Any]] = []
    dump_dir = Path("src/dump")
    try:
        if not dump_dir.exists():
            return []
        session_files = list(dump_dir.glob("session_*.json"))
        for sf in session_files:
            try:
                with open(sf, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                agents = data.get('agents', [])
                total_agents = 15
                completed_agents = len([a for a in agents if a.get('status') == 'completed'])
                progress = (completed_agents / total_agents) * 100 if total_agents > 0 else 0
                raw_status = (data.get('status') or '').lower()
                # æ¨å¯¼æ›´ç¨³å¥çš„ä»»åŠ¡çŠ¶æ€
                if raw_status == 'completed' or completed_agents >= total_agents:
                    status = 'completed'
                elif raw_status == 'cancelled':
                    status = 'cancelled'
                else:
                    if any((a.get('status') or '').lower() == 'running' for a in agents):
                        status = 'running'
                    elif agents and completed_agents < total_agents:
                        status = 'running'
                    else:
                        status = raw_status or 'unknown'
                user_query = (data.get('user_query') or '').strip()
                session_id = data.get('session_id', sf.stem)
                created_at = data.get('created_at', '')
                # è§£ææ—¶é—´
                try:
                    created_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00')) if created_at else datetime.fromtimestamp(sf.stat().st_mtime)
                except Exception:
                    created_dt = datetime.fromtimestamp(sf.stat().st_mtime)
                sessions_info.append({
                    'file': str(sf),
                    'session_id': session_id,
                    'user_query': user_query,
                    'status': status,
                    'completed': completed_agents,
                    'total': total_agents,
                    'progress': progress,
                    'created_at': created_dt,
                    'mtime': sf.stat().st_mtime,
                })
            except Exception:
                continue
        # æœ€æ–°åœ¨å‰
        sessions_info.sort(key=lambda x: x['mtime'], reverse=True)
        return sessions_info
    except Exception:
        return []


def show_tasks_overview():
    """å±•ç¤ºå½“å‰ä»»åŠ¡è¿›åº¦ï¼ˆå¤šä»»åŠ¡ï¼‰ã€‚"""
    st.markdown("### ğŸ§µ å½“å‰ä»»åŠ¡è¿›åº¦")
    sessions = get_all_sessions_progress()
    if not sessions:
        st.info("å½“å‰æ²¡æœ‰ä¼šè¯ä»»åŠ¡è®°å½•")
        return

    # ä»…æ˜¾ç¤ºâ€œæ­£åœ¨è¿è¡Œâ€çš„ä»»åŠ¡ï¼Œå¹¶ä¸”é™å®šä¸ºæœ€è¿‘ä¸€æ®µæ—¶é—´å†…æ´»è·ƒçš„ä¼šè¯ï¼ˆæ ¹æ®æ–‡ä»¶ä¿®æ”¹æ—¶é—´åˆ¤æ–­ï¼‰
    recent_minutes = 3  # è®¤ä¸º3åˆ†é’Ÿå†…ä¿®æ”¹çš„ä¼šè¯ä»åœ¨æ´»è·ƒ
    now_ts = datetime.now().timestamp()
    filtered = [
        s for s in sessions
        if ((s['status'] == 'running') or (s['progress'] < 100 and s['status'] not in ('completed', 'cancelled')))
        and (now_ts - s['mtime']) <= recent_minutes * 60
    ]
    if not filtered:
        st.info("æš‚æ— è¿›è¡Œä¸­çš„ä»»åŠ¡")
        return

    for s in filtered[:20]:  # æœ€å¤šæ˜¾ç¤º20æ¡ï¼Œé¿å…è¿‡é•¿
        q = s['user_query'] or s['session_id']
        title = q if len(q) <= 50 else q[:50] + '...'
        c1, c2, c3, c4 = st.columns([3, 2, 4, 1])
        with c1:
            st.markdown(f"**{title}**")
            st.caption(s['created_at'].strftime('%m-%d %H:%M'))
        with c2:
            emoji = "âœ…" if s['status'] == 'completed' else "ğŸ”„" if s['status'] == 'running' else "â³"
            st.markdown(f"{emoji} {s['status']}")
            st.caption(f"{s['completed']}/{s['total']}")
        with c3:
            st.progress(min(max(s['progress'], 0), 100) / 100.0)
        with c4:
            if st.button("æŸ¥çœ‹", key=f"view_{s['session_id']}"):
                load_session_data(s['file'])
                st.rerun()


def get_max_concurrent_limit() -> int:
    """ä» .env è¯»å–æœ€å¤§å¹¶å‘ä»»åŠ¡ä¸Šé™ï¼Œé»˜è®¤ 2ã€‚"""
    try:
        val = os.getenv("MAX_CONCURRENT_ANALYSIS", "2").strip()
        limit = int(val)
        return max(1, limit)
    except Exception:
        return 2


def get_current_running_tasks_count() -> int:
    """ç»Ÿè®¡å½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ•°é‡ï¼ˆä½¿ç”¨ä¸ä»»åŠ¡åˆ—è¡¨ä¸€è‡´çš„åˆ¤å®šé€»è¾‘ï¼‰ã€‚"""
    sessions = get_all_sessions_progress()
    if not sessions:
        return 0
    now_ts = datetime.now().timestamp()
    # ä½¿ç”¨ä¸ show_tasks_overview ç›¸åŒçš„ 3 åˆ†é’Ÿæ´»è·ƒçª—å£
    recent_minutes = 3
    running = [
        s for s in sessions
        if ((s['status'] == 'running') or (s['progress'] < 100 and s['status'] not in ('completed', 'cancelled')))
        and (now_ts - s['mtime']) <= recent_minutes * 60
    ]
    return len(running)


def start_analysis(query: str):
    """å¼€å§‹åˆ†æ"""
    # æ£€æŸ¥è¿æ¥çŠ¶æ€
    if not st.session_state.get('orchestrator'):
        st.error("ç³»ç»Ÿæœªè¿æ¥ï¼Œæ— æ³•å¼€å§‹åˆ†æ")
        return
    
    # å¹¶å‘ä¸Šé™æ§åˆ¶
    max_limit = get_max_concurrent_limit()
    running_count = get_current_running_tasks_count()
    if running_count >= max_limit:
        st.warning(f"å½“å‰è¿è¡Œä¸­çš„ä»»åŠ¡å·²è¾¾ä¸Šé™ï¼ˆ{running_count}/{max_limit}ï¼‰ï¼Œè¯·ç¨åå†è¯•æˆ–ç­‰å¾…ä»»åŠ¡å®Œæˆ")
        return
    
    # é‡ç½®çŠ¶æ€
    st.session_state.analysis_running = True
    st.session_state.analysis_completed = False
    st.session_state.analysis_cancelled = False
    
    # å°†orchestratorä¼ é€’ç»™åˆ†æå‡½æ•°
    run_analysis_sync(query, st.session_state.orchestrator)


def run_analysis_sync(query: str, orchestrator):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æï¼Œé¿å…é˜»å¡Streamlitä¸»çº¿ç¨‹"""
    import threading
    
    class AnalysisState:
        def __init__(self):
            self.cancelled = False
            self.running = True
            self.completed = False
            self.result = None
            self.error = None
    
    # åˆ›å»ºçŠ¶æ€å¯¹è±¡
    analysis_state = AnalysisState()
    
    def run_analysis_thread():
        """åå°çº¿ç¨‹æ‰§è¡Œåˆ†æ"""
        try:
            load_dotenv()
            
            # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if analysis_state.cancelled:
                analysis_state.running = False
                return
            
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(run_single_analysis_async_safe(query, orchestrator, analysis_state))
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
                if analysis_state.cancelled:
                    analysis_state.running = False
                    return
                
                # åˆ†ææˆåŠŸ
                analysis_state.result = result
                analysis_state.completed = True
                analysis_state.running = False
                
            finally:
                loop.close()
                
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å–æ¶ˆå¯¼è‡´çš„å¼‚å¸¸
            if not analysis_state.cancelled:
                error_msg = str(e)
                analysis_state.error = error_msg
            analysis_state.running = False
            analysis_state.completed = False
    
    # å¯åŠ¨åå°çº¿ç¨‹
    thread = threading.Thread(target=run_analysis_thread, daemon=True)
    thread.start()
    
    # å­˜å‚¨çº¿ç¨‹å¼•ç”¨å’ŒçŠ¶æ€å¯¹è±¡
    st.session_state.analysis_thread = thread
    st.session_state.analysis_state_obj = analysis_state


async def run_single_analysis_async_safe(user_query: str, orchestrator, analysis_state) -> Optional[dict]:
    """å®‰å…¨çš„å¼‚æ­¥åˆ†æå‡½æ•°"""
    try:
        # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if analysis_state.cancelled:
            return None
            
        workflow_info = orchestrator.get_workflow_info()
        enabled_agents = orchestrator.get_enabled_agents()
        
        # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if analysis_state.cancelled:
            return None
        
        # åˆ›å»ºå–æ¶ˆæ£€æŸ¥å™¨å‡½æ•°
        def cancel_checker():
            return analysis_state.cancelled
        
        result = await orchestrator.run_analysis(user_query, cancel_checker)
        
        # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if analysis_state.cancelled:
            return None
        
        return result
        
    except Exception as e:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å–æ¶ˆå¯¼è‡´çš„å¼‚å¸¸
        if analysis_state.cancelled:
            return None
        raise


def stop_analysis():
    """åœæ­¢æ­£åœ¨è¿è¡Œçš„åˆ†æ"""
    st.session_state.analysis_cancelled = True
    st.session_state.analysis_running = False
    
    # è®¾ç½®çŠ¶æ€å¯¹è±¡çš„å–æ¶ˆæ ‡å¿—
    analysis_state_obj = st.session_state.get('analysis_state_obj')
    if analysis_state_obj:
        analysis_state_obj.cancelled = True
        analysis_state_obj.running = False


def main():
    """ä¸»ç•Œé¢ - ç²¾ç®€ä¿¡æ¯æ¶æ„ï¼Œç»“æœä¼˜å…ˆ"""
    # åŠ è½½æ ·å¼
    load_page_styles()
    
    # æ˜¾ç¤ºè´´é¡¶æŠ¬å¤´ï¼ˆç´§è´´é¡µé¢æœ€ä¸Šæ–¹ï¼‰
    render_top_header()
    
    
    # é‡‡ç”¨ä¸‰æ®µå¼ç»“æ„ï¼šå…³é”®æ“ä½œåŒºï¼ˆä¸Šï¼‰â†’ å·¥ä½œåŒºï¼ˆä¸­ï¼‰â†’ ç»“æœä¸å¯¼å‡ºï¼ˆä¸‹ï¼‰
    st.markdown("---")

    # 1) å…³é”®æ“ä½œåŒºï¼šè¾“å…¥ + å¿«é€ŸçŠ¶æ€
    op_c1, op_c2, op_c3 = st.columns([1, 1, 1])
    with op_c1:
        st.markdown("### ğŸ” å®æ—¶åˆ†æ")
        show_real_time_analysis()
    with op_c2:
        st.markdown("### ğŸ“š å†å²ä¼šè¯")
        show_history_management()
    with op_c3:
        st.markdown("### ğŸ§­ ç³»ç»ŸçŠ¶æ€")
        env_status = "âœ…" if Path(".env").exists() else "âŒ"
        mcp_status = "âœ…" if Path("mcp_config.json").exists() else "âŒ"
        status_c1, status_c2 = st.columns(2)
        with status_c1:
            st.metric("ç¯å¢ƒ", env_status)
        with status_c2:
            st.metric("MCP", mcp_status)

    # 2) å¤šä»»åŠ¡è¿›åº¦æ€»è§ˆ
    st.markdown("---")
    show_tasks_overview()

    # 3) ç»“æœä¸å¯¼å‡º
    st.markdown("---")
    res_c1, res_c2 = st.columns([3, 1])
    with res_c1:
        st.markdown("### ğŸ“ˆ åˆ†æç»“æœ")
        show_analysis_results()
    with res_c2:
        st.markdown("### ğŸ“¤ æŠ¥å‘Šå¯¼å‡º")
        with st.expander("ğŸ“Œ æ“ä½œè¯´æ˜ï¼ˆå¯æ”¶èµ·ï¼‰", expanded=False):
            st.caption("è¾“å…¥æŸ¥è¯¢åç‚¹å‡»â€˜å¼€å§‹åˆ†æâ€™ï¼Œå†å²ä¼šè¯å¯ç›´æ¥åˆ‡æ¢æŸ¥çœ‹ç»“æœï¼›å¯¼å‡ºåœ¨æœ¬åŒºâ€˜æŠ¥å‘Šå¯¼å‡ºâ€™æ¨¡å—ã€‚")
        show_export_options()


    # ç³»ç»Ÿæ¦‚è§ˆç§»è‡³é¡µé¢åº•éƒ¨ï¼Œé¿å…æ‰“æ–­ä¸»æµç¨‹
    st.markdown("---")
    with st.expander("ğŸ›ï¸ AIäº¤æ˜“åˆ†æå®éªŒå®¤ - ç³»ç»Ÿæ¦‚è§ˆ", expanded=False):
        show_system_overview()


if __name__ == "__main__":
    main()
